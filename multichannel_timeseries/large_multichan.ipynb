{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Multi-Channel Timeseries with Large Datasets Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/large_multichan-ts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introduction, please visit the ['Index'](./index.ipynb) page. This workflow is tailored for processing and analyzing large-sized multi-channel timeseries data derived from [electrophysiological](https://en.wikipedia.org/wiki/Electrophysiology) recordings. It is more experimental and complex than the other related workflow approaches, but provides a scalable solutiom.\n",
    "\n",
    "### What Defines a 'Large-Sized' Dataset?\n",
    "\n",
    "A 'large-sized' dataset in this context is characterized by its size surpassing the available memory, making it impossible to load the entire dataset into RAM simultaneously. So, how are we to visualize a zoomed-out representation of the entire large dataset?\n",
    "\n",
    "### Utilizing a Large Data Pyramid\n",
    "\n",
    "In the 'medium' workflow, we employed downsampling to reduce the volume of data transferred to the browser, a technique feasible when the entire dataset already resides in memory. For larger datasets, however, we now adopt an additional strategy: the creation and dynamic access to a data pyramid. A data pyramid involves storing multiple layers of the dataset at varying resolutions, where each successive layer is a downsampled version of the previous one. For instance, when fully zoomed out, a greatly downsampled version of the data provides a quick overview, guiding users to areas of interest. Upon zooming in, tiles of higher-resolution pyramid levels are dynamically loaded. This strategy outlined is similar to the approach used in geosciences for managing interactive map tiles, and which has also been adopted in bio-imaging for handling high-resolution electron microscopy images. \n",
    "\n",
    "### Key Software:\n",
    "\n",
    "Alongside [HoloViz](https://github.com/holoviz), [Bokeh](https://holoviz.org/), and [Numpy](https://numpy.org/), we make extensive use of several open source libraries to implement our solution:\n",
    "- **[Xarray](https://github.com/pydata/xarray):** Manages labeled multi-dimensional data, facilitating complex data operations and enabling partial data loading for out-of-core computation.\n",
    "- **[Xarray DataTree](https://github.com/xarray-contrib/datatree):** Organizes xarray DataArrays and Datasets into a logical tree structure, making it easier to manage and access different resolutions of the dataset. At the moment of writing, this is [actively being migrated](https://github.com/pydata/xarray/issues/8572) into the core Xarray library.\n",
    "- **[Dask](https://github.com/dask/dask):** Adds parallel computing capabilities, managing tasks that exceed memory limits.\n",
    "- **[ndpyramid](https://github.com/carbonplan/ndpyramid):** Specifically designed for creating multi-resolution data pyramids.\n",
    "- **[Zarr](https://github.com/zarr-developers/zarr-python):** Used for storing the large arrays of the data pyramid on disk in a compressed, chunked, and memory-mappable format, which is crucial for efficient data retrieval.\n",
    "- **[tsdownsample](https://github.com/predict-idlab/tsdownsample):** Provides optimized implementations of downsampling algorithms that help to maintain important aspects of the data.\n",
    "\n",
    "### Considerations and Trade-offs\n",
    "While this approach allows visualization and interaction with datasets larger than available memory, it does introduce certain trade-offs:\n",
    "\n",
    "- **Increased Storage Requirement:** Constructing a data pyramid currently requires additional disk space since multiple representations of the data are stored.\n",
    "- **Code Complexity:** Creating the pyramids still involves a fair bit of familiarity with the key packages, and their interoperability. Also, the plotting code involved in dynamic access to the data pyramid structure is still experimental, and could be matured into HoloViz or another codebase in the future.\n",
    "- **Performance:** While this method can handle large datasets, the performance may not match that of handling smaller datasets due to the overhead associated with processing and dynamically loading multiple layers of the pyramid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Resources\n",
    "\n",
    "| Topic | Type | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro and Guidance](./index.ipynb) | Prerequisite | Background |\n",
    "| [Time Range Annotation](./time_range_annotation.ipynb) | Next Step | Display and edit time ranges |\n",
    "| [Smaller Dataset Workflow](./small_multi-chan-ts.ipynb) | Alternative | Use Numpy |\n",
    "| [Medium Dataset Workflow](./medium_multi-chan-ts.ipynb) | Alternative | Use Pandas and downsampling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from xarray.backends.api import open_datatree # until API stabilizes\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "\n",
    "from ndpyramid import pyramid_create\n",
    "from tsdownsample import MinMaxLTTBDownsampler\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension(\"tabulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Utilize a smaller version of the dataset, but provide instructions for accessing a larger copy of the data\n",
    "  # add text about data (3GB) access: s3://datasets.holoviz.org/ephys_sim/v1/ephys_sim_neuropixels_10s_384ch.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OVERWRITE = True # Set True once initially create data pyramid\n",
    "\n",
    "# Dataset-specific parameters\n",
    "\n",
    "# Option 1: Simulated neuropixels spike-band dataset\n",
    "DATA_DIR = Path('~/data/ephys_sim_neuropixels/').expanduser()\n",
    "H5_FILE = Path('ephys_sim_neuropixels_10s_384ch.h5')\n",
    "DATA_KEY = \"recordings\"\n",
    "DATA_DIMS = { # Each dim item value should be the path to the data in the h5 file\n",
    "    \"time\": \"timestamps\",\n",
    "    \"channel\": \"channels\",\n",
    "}\n",
    "\n",
    "# Option 2: Neuropixels LFP-band dataset from allen institute\n",
    "# DATA_DIR = Path(\"~/data/allen/\").expanduser()\n",
    "# H5_FILE = Path(\"probe_810755797_lfp.nwb\")\n",
    "# DATA_KEY = \"acquisition/probe_810755797_lfp_data/data\"\n",
    "# DATA_DIMS = {\n",
    "#     \"time\": \"acquisition/probe_810755797_lfp_data/timestamps\",\n",
    "#     \"channel\": \"acquisition/probe_810755797_lfp_data/electrodes\",\n",
    "# }\n",
    "\n",
    "# TODO: remove max channel limits before final publishing\n",
    "MAX_CHANNELS_TO_PROCESS = 100\n",
    "MAX_CHANNELS_TO_DISPLAY = 50\n",
    "\n",
    "# Common parameters\n",
    "H5_PATH = DATA_DIR / H5_FILE\n",
    "PYRAMID_FILE = f\"{H5_FILE.stem}.zarr\"\n",
    "PYRAMID_PATH = DATA_DIR / PYRAMID_FILE\n",
    "print('Pyramid Path:', PYRAMID_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to `xarray.DataArray`\n",
    "\n",
    "Before building a data pyramid, we'll first we construct an `xarray.DataArray` version of our dataset from its original HDF5 format. We'll make use of `Dask` for parallel and 'lazy' computation, i.e. chunks of the data are only loaded when necessary, enabling operations on data that exceed memory limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_to_xarray(f, data_key, dims):\n",
    "    \"\"\"Serialize HDF5 data into an xarray Dataset with lazy loading.\"\"\"\n",
    "    # Extract coordinates for the specified dimensions\n",
    "    coords = {dim: f[coord_key][:] for dim, coord_key in dims.items()}\n",
    "    \n",
    "    # Load the dataset lazily using Dask\n",
    "    data = f[data_key]\n",
    "    dask_data = da.from_array(data, chunks=(data.shape[0], 1))\n",
    "    \n",
    "    # Create the xarray DataArray and convert it to a Dataset\n",
    "    data_array = xr.DataArray(\n",
    "        dask_data,\n",
    "        dims=list(dims.keys()),\n",
    "        coords=coords,\n",
    "        name=data_key.split(\"/\")[-1]\n",
    "    )\n",
    "    ds = data_array.to_dataset(name='data') #data_key.split(\"/\")[-1]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(H5_PATH, \"r\")\n",
    "ts_ds = serialize_to_xarray(f, DATA_KEY, DATA_DIMS).isel(channel=slice(MAX_CHANNELS_TO_PROCESS))\n",
    "ts_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Pyramid\n",
    "\n",
    "We will feed our new `xarray.DataArray` to `ndpyramid.pyramid_create`, also passing in the dimension that we want downsampled ('`time`'), a custom `apply_downsample` function that uses `xarray.apply_ufunc` to perform computations in a vectorized and parallelized manner, and `FACTORS` which determine the extent of each downsampled level. For instance, a factor of '2' halves the number of time samples, '4' reduces them to a quarter, and so on.\n",
    "\n",
    "To each chunk of data, our custom `apply_downsample` function applies the `MinMaxLTTBDownsampler` from the `tsdownsample` library, which selects data points that best represent the overall shape of the signal. This method is particularly effective in preserving the visual integrity of the data, even at reduced resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTORS = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# TODO: find better principled way to determine factors.. The following doesn't work as the number of channels scales\n",
    "# FACTORS = list(np.array([1, 2, 4, 8, 16, 32, 64, 128, 256]) ** (len(ts_ds[\"channel\"]) // 4))\n",
    "\n",
    "def _help_downsample(data, time, n_out):\n",
    "    \"\"\"\n",
    "    Helper function for downsampling and returning as a specific format.\n",
    "    \"\"\"\n",
    "    indices = MinMaxLTTBDownsampler().downsample(time, data, n_out=n_out)\n",
    "    return data[indices], indices\n",
    "\n",
    "\n",
    "def apply_downsample(ts_ds, factor, dims):\n",
    "    \"\"\"\n",
    "    Apply downsampling to a time series dataset.\n",
    "    \"\"\"\n",
    "    dim = dims[0]\n",
    "    n_out = len(ts_ds[\"data\"]) // factor\n",
    "    print(f\"Downsampling by factor {factor} for a size of {n_out}.\")\n",
    "    ts_ds_downsampled, indices = xr.apply_ufunc(\n",
    "        _help_downsample,\n",
    "        ts_ds[\"data\"],\n",
    "        ts_ds[dim],\n",
    "        kwargs=dict(n_out=n_out),\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        output_core_dims=[[dim], [\"indices\"]],\n",
    "        exclude_dims=set((dim,)),\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs=dict(output_sizes={dim: n_out, \"indices\": n_out}),\n",
    "    )\n",
    "    # Update the dimension coordinates with the downsampled indices\n",
    "    ts_ds_downsampled[dim] = ts_ds[dim].isel(time=indices.values[0])\n",
    "    return ts_ds_downsampled.rename(\"data\")\n",
    "\n",
    "\n",
    "if not PYRAMID_PATH.exists() or OVERWRITE:\n",
    "    ts_dt = pyramid_create(\n",
    "        ts_ds,\n",
    "        factors=FACTORS,\n",
    "        dims=[\"time\"],\n",
    "        func=apply_downsample,\n",
    "        type_label=\"pick\",\n",
    "        method_label=\"pyramid_downsample\",\n",
    "    )\n",
    "    display(ts_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist and Re-open\n",
    "\n",
    "Now we can easily save the multi-level pyramid `to_zarr` format on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PYRAMID_PATH.exists() or OVERWRITE:\n",
    "    PYRAMID_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ts_dt.to_zarr(PYRAMID_PATH, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And read it back in just as easily; just be sure to specify the `zarr` engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dt = open_datatree(PYRAMID_PATH, engine=\"zarr\")\n",
    "ts_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you expand the 'Group' dropdown above, you can see each pyramid level has the same number of channels, but different number of timestamps, since the time dimension was downsampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Pyramid Plotting\n",
    "\n",
    "Now that we've created our data pyramid, we can set up the interactive visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "First, we will prepare some metadata needed for plotting and define a helper function to extract a dataset at a specific pyramid level and channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ds(ts_dt, level, channels=None):\n",
    "    \"\"\" Extract a dataset at a specific level\"\"\"\n",
    "    ds = ts_dt[str(level)].ds\n",
    "    return ds if channels is None else ds.sel(channel=channels)\n",
    "\n",
    "# Grab the timestamps from the coursest level of the datatree for initialization\n",
    "num_levels = len(ts_dt)\n",
    "coarsest_level = str(num_levels-1)\n",
    "time_da = _extract_ds(ts_dt, coarsest_level)[\"time\"]\n",
    "channels = ts_dt[coarsest_level].ds[\"channel\"].values[:MAX_CHANNELS_TO_DISPLAY]\n",
    "num_channels = len(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dynamic Plot\n",
    "\n",
    "We'll utilize a HoloViews `DynamicMap` which will call our custom function called `rescale` whenever there is a change in the visible axes' ranges (`RangeXY`) or the size of a plot (`PlotSize`).\n",
    "\n",
    "Based on the changes and thresholds, a new plot is created using a new subset of the datatree pyramid.\n",
    "\n",
    "\n",
    "<details> <summary> <strong>Want more details? Click here</strong> </summary>\n",
    "\n",
    "When the `rescale` function is triggered, it will first determine which pyramid `zoom_level` has the next closest number of data samples in the visible time range (`time_slice`) compared with the number of horizontal pixels on the screen.\n",
    "\n",
    "Depending on the determined `zoom_level`, data corresponding to the visible time range is fetched through the `_extract_ds` function, which accesses the specific slice of data from the appropriate pyramid level.\n",
    "\n",
    "Finally, for each channel within the specified range, a `Curve` element is generated using HoloViews, and each curve is added to the `Overlay` for a stacked multi-channel timeseries visualization.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PADDING = 0.2  # buffer x-range to reduce update latency with pans and zoom-outs\n",
    "\n",
    "amplitude_dim = hv.Dimension(\"amplitude\", unit=\"µV\")\n",
    "time_dim = hv.Dimension(\"time\", unit=\"s\") # match the index name in the df\n",
    "\n",
    "def rescale(x_range, y_range, width, scale, height):\n",
    "\n",
    "    # Handle edge case when streams are initialized\n",
    "    if x_range is None:\n",
    "        x_range = time_da.min().item(), time_da.max().item()\n",
    "    if y_range is None:\n",
    "        y_range = 0, num_channels\n",
    "\n",
    "    # define time range slice\n",
    "    x_padding = (x_range[1] - x_range[0]) * X_PADDING\n",
    "    time_slice = slice(x_range[0] - x_padding, x_range[1] + x_padding)\n",
    "    channel_slice = slice(y_range[0], y_range[1])\n",
    "\n",
    "    # calculate the appropriate pyramid level and size\n",
    "    if width is None or height is None:\n",
    "        pyramid_level = num_levels - 1\n",
    "        size = time_da.size\n",
    "    else:\n",
    "        sizes = np.array([\n",
    "            _extract_ds(ts_dt, pyramid_level)[\"time\"].sel(time=time_slice).size\n",
    "            for pyramid_level in range(num_levels)\n",
    "        ])\n",
    "        diffs = sizes - width\n",
    "        pyramid_level = np.argmin(np.where(diffs >= 0, diffs, np.inf)) # nearest higher-resolution level\n",
    "        # pyramid_level = np.argmin(np.abs(np.array(sizes) - width)) # nearest, regardless of direction\n",
    "        size = sizes[pyramid_level]\n",
    "    \n",
    "    title = (\n",
    "        f\"[Pyramid Level {pyramid_level} ({x_range[0]:.2f}s - {x_range[1]:.2f}s)]   \"\n",
    "        f\"[Time Samples: {size}]  [Plot Size WxH: {width}x{height}]\"\n",
    "    )\n",
    "\n",
    "    # extract new data and re-paint the plot\n",
    "    ds = _extract_ds(ts_dt, pyramid_level, channels).sel(time=time_slice, channel=channel_slice).load()\n",
    "\n",
    "    curves = {}\n",
    "    for channel in ds[\"channel\"].values.tolist():\n",
    "        curves[str(channel)] = hv.Curve(ds.sel(channel=channel), [time_dim], ['data'], label=str(channel)).redim(\n",
    "            data=amplitude_dim).opts(\n",
    "            color=\"black\",\n",
    "            line_width=1,\n",
    "            subcoordinate_y=True,\n",
    "            subcoordinate_scale=2,\n",
    "            hover_tooltips = [\n",
    "                (\"channel\", \"$label\"),\n",
    "                (\"time\"),\n",
    "                (\"amplitude\")],\n",
    "            tools=[\"xwheel_zoom\"],\n",
    "            active_tools=[\"box_zoom\"],\n",
    "        )\n",
    "        \n",
    "    curves_overlay = hv.NdOverlay(curves, kdims=\"Channel\", sort=False).opts(\n",
    "            xlabel=\"Time (s)\",\n",
    "            ylabel=\"Channel\",\n",
    "            title=title,\n",
    "            show_legend=False,\n",
    "            padding=0,\n",
    "            min_height=600,\n",
    "            responsive=True,\n",
    "            framewise=True,\n",
    "            axiswise=True,\n",
    "        )\n",
    "    return curves_overlay\n",
    "\n",
    "range_stream = hv.streams.RangeXY()\n",
    "size_stream = hv.streams.PlotSize()\n",
    "dmap = hv.DynamicMap(rescale, streams=[size_stream, range_stream])\n",
    "\n",
    "# dmap # uncomment to display the curves plot without further extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimap Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "\n",
    "y_positions = range(num_channels)\n",
    "yticks = [(i, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "z_data = zscore(ts_dt[coarsest_level].ds[\"data\"].values[:MAX_CHANNELS_TO_DISPLAY], axis=1)\n",
    "\n",
    "minimap = rasterize(\n",
    "    hv.QuadMesh((time_da, y_positions, z_data), [\"Time\", \"Channel\"], \"Amplitude\")\n",
    ")\n",
    "\n",
    "minimap = minimap.opts(\n",
    "    cnorm='eq_hist',\n",
    "    cmap=\"RdBu_r\",\n",
    "    alpha=0.5,\n",
    "    xlabel=\"\",\n",
    "    yticks=[yticks[0], yticks[-1]],\n",
    "    toolbar=\"disable\",\n",
    "    height=120,\n",
    "    responsive=True,\n",
    ")\n",
    "\n",
    "tool_link = RangeToolLink(\n",
    "    minimap,\n",
    "    dmap,\n",
    "    axes=[\"x\", \"y\"],\n",
    "    boundsx=(0, time_da.max().item() // 2),\n",
    "    boundsy=(0, len(channels) // 2),\n",
    ")\n",
    "\n",
    "nb_app = (dmap + minimap).cols(1)\n",
    "nb_app # uncomment to display app in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone App Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_app = pn.template.FastListTemplate(\n",
    "    title = \"HoloViz + Bokeh Multi-Channel Timeseries with Large Data via Pyramid\",\n",
    "    main = pn.Column(nb_app),\n",
    ").servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
