{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Multichannel Timeseries with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 2px solid #ccc; border-radius: 8px; padding: 5px; display: inline-block;\">\n",
    "    <img src=\"./assets/large_multichan-ts.png\" alt=\"Header Image\" style=\"display: block; max-width: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center; padding: 10px;\">\n",
    "    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bJf58B7SRA0?si=XLZ3unzoQXPWGzUA\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| What? | Why? |\n",
    "| --- | --- |\n",
    "| [Index: Intro, Workflows, Extensions](./index.ipynb) | For context and workflow selection/feature guidance |\n",
    "| [Recommended Workflow](./multichan.ipynb) | For live downsampling with in-memory Pandas DataFrame |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introduction, please visit the ['Index'](./index.ipynb) page. This workflow is tailored for processing and analyzing large multichannel timeseries data derived from [electrophysiological](https://en.wikipedia.org/wiki/Electrophysiology) recordings. It is more experimental and complex than the recommended workflow but, at the time of writing, provides a scalable solution at the cutting edge of development.\n",
    "\n",
    "### What Defines a 'Large-Sized' Dataset?\n",
    "\n",
    "A 'large-sized' dataset in this context is characterized by its size surpassing the available memory, making it impossible to load the entire dataset into RAM simultaneously. So, how are we to visualize a zoomed-out representation of the entire large dataset?\n",
    "\n",
    "### Utilizing a Large Data Pyramid\n",
    "\n",
    "In the 'medium' workflow, we employed downsampling to reduce the volume of data transferred to the browser, a technique feasible when the entire dataset already resides in memory. For larger datasets, however, we now adopt an additional strategy: the creation and dynamic access to a data pyramid. A data pyramid involves storing multiple layers of the dataset at varying resolutions, where each successive layer is a downsampled version of the previous one. For instance, when fully zoomed out, a greatly downsampled version of the data provides a quick overview, guiding users to areas of interest. Upon zooming in, tiles of higher-resolution pyramid levels are dynamically loaded. This strategy outlined is similar to the approach used in geosciences for managing interactive map tiles, and which has also been adopted in bio-imaging for handling high-resolution electron microscopy images. \n",
    "\n",
    "### Key Software:\n",
    "\n",
    "Alongside [HoloViz](https://github.com/holoviz), [Bokeh](https://holoviz.org/), and [Numpy](https://numpy.org/), we make extensive use of several open source libraries to implement our solution:\n",
    "- **[Xarray](https://github.com/pydata/xarray):** Manages labeled multi-dimensional data, facilitating complex data operations and enabling partial data loading for out-of-core computation.\n",
    "- **[Xarray DataTree](https://github.com/xarray-contrib/datatree):** Organizes xarray DataArrays and Datasets into a logical tree structure, making it easier to manage and access different resolutions of the dataset. At the moment of writing, this is [actively being migrated](https://github.com/pydata/xarray/issues/8572) into the core Xarray library.\n",
    "- **[Dask](https://github.com/dask/dask):** Adds parallel computing capabilities, managing tasks that exceed memory limits.\n",
    "- **[ndpyramid](https://github.com/carbonplan/ndpyramid):** Specifically designed for creating multi-resolution data pyramids.\n",
    "- **[Zarr](https://github.com/zarr-developers/zarr-python):** Used for storing the large arrays of the data pyramid on disk in a compressed, chunked, and memory-mappable format, which is crucial for efficient data retrieval.\n",
    "- **[tsdownsample](https://github.com/predict-idlab/tsdownsample):** Provides optimized implementations of downsampling algorithms that help to maintain important aspects of the data.\n",
    "\n",
    "### Considerations and Trade-offs\n",
    "While this approach allows visualization and interaction with datasets larger than available memory, it does introduce certain trade-offs:\n",
    "\n",
    "- **Increased Storage Requirement:** Constructing a data pyramid currently requires additional disk space since multiple representations of the data are stored.\n",
    "- **Code Complexity:** Creating the pyramids still involves a fair bit of familiarity with the key packages, and their interoperability. Also, the plotting code involved in dynamic access to the data pyramid structure is still experimental, and could be matured into HoloViz or another codebase in the future.\n",
    "- **Performance:** While this method can handle large datasets, the performance may not match that of handling smaller datasets due to the overhead associated with processing and dynamically loading multiple layers of the pyramid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from xarray.backends.api import open_datatree # until API stabilizes\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "\n",
    "from ndpyramid import pyramid_create\n",
    "from tsdownsample import MinMaxLTTBDownsampler\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://datasets.holoviz.org/lfp/v1/sub-719828686_ses-754312389_probe-756781563_ecephys.nwb'\n",
    "DATA_DIR = Path('./data')\n",
    "DATA_FILENAME = Path(DATA_URL).name\n",
    "DATA_PATH = DATA_DIR / DATA_FILENAME\n",
    "PYRAMID_FILE = f\"{DATA_PATH.stem}.zarr\"\n",
    "PYRAMID_PATH = DATA_DIR / PYRAMID_FILE\n",
    "print(f'Local Original Data Path: {DATA_PATH}')\n",
    "print(f'Pyramid Path: {PYRAMID_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-warning\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Warning</p>\n",
    "    The following cell will download ~900 MB the first time it is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not DATA_PATH.exists():\n",
    "    import wget\n",
    "    print(f'Data downloading to: {DATA_PATH}')\n",
    "    wget.download(DATA_URL, out=str(DATA_PATH))\n",
    "else:\n",
    "    print(f'Data exists at: {DATA_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Raw File\n",
    "\n",
    "In order to select the relevant parts of the original dataset that we are interested in, we first have to figure out how where they are in the database. The original NWB (HDF5) file format is like a file-system within a file. According to the [NWB docs](https://nwb-overview.readthedocs.io/en/latest/intro_to_nwb/2_file_structure.html#file-hierarchy), we should find the data in either the '`/acquisition`' or '`processing`' path groups. Let's write a function to quickly display everything in those groups. We are looking for 'LFP' data which will be a 2D array and the corresponding LFP data dimension vectors (time, electrodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hdf5_keys(filename, path='/'):\n",
    "    \"\"\"Prints all keys (paths) in an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        for key in f[path].keys():\n",
    "            full_path = f\"{path}/{key}\" if path != '/' else key\n",
    "            print(full_path)\n",
    "            if isinstance(f[full_path], h5py.Group):\n",
    "                print_hdf5_keys(filename, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hdf5_keys(DATA_PATH, path='/processing')\n",
    "print_hdf5_keys(DATA_PATH, path='/acquisition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our LFP `data` key is in `acquisition/probe_810755797_lfp_data`, along with the `electrodes` and `timestamps` dimensions. Don't worry about the multiple `lfp.../data` files, these paths point to the same underlying object. Let's note down the paths to our data objects of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_KEY = \"acquisition/probe_756781563_lfp_data/data\"\n",
    "DATA_DIMS = {\"time\": \"acquisition/probe_756781563_lfp_data/timestamps\",\n",
    "             \"channel\": \"acquisition/probe_756781563_lfp_data/electrodes\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Intermediate `Xarray` Dataset\n",
    "\n",
    "Before building a data pyramid, we'll first we construct an `Xarray` version of our dataset from its original NWB (HDF5) format. We'll make use of `Dask` for parallel and 'lazy' computation, i.e. chunks of the data are only loaded when necessary, enabling operations on data that exceed memory limits. An impactful goal for future work would be to create the data pyramid directly from original NWB (HDF5) file and skip creation of this intermediate version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_to_xarray(f, data_key, dims):\n",
    "    \"\"\"Serialize HDF5 data into an xarray Dataset with lazy loading.\"\"\"\n",
    "    # Extract coordinates for the specified dimensions\n",
    "    coords = {dim: f[coord_key][:] for dim, coord_key in dims.items()}\n",
    "    \n",
    "    # Load the dataset lazily using Dask\n",
    "    data = f[data_key]\n",
    "    dask_data = da.from_array(data, chunks=(data.shape[0], 1))\n",
    "    \n",
    "    # Create the xarray DataArray and convert it to a Dataset\n",
    "    data_array = xr.DataArray(\n",
    "        dask_data,\n",
    "        dims=list(dims.keys()),\n",
    "        coords=coords,\n",
    "        name=data_key.split(\"/\")[-1]\n",
    "    )\n",
    "    ds = data_array.to_dataset(name='data') #data_key.split(\"/\")[-1]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(DATA_PATH, \"r\")\n",
    "ts_ds = serialize_to_xarray(f, DATA_KEY, DATA_DIMS)\n",
    "ts_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Pyramid\n",
    "\n",
    "We will feed our new `Xarray` data to `ndpyramid.pyramid_create`, also passing in the dimension that we want downsampled ('`time`'), a custom `apply_downsample` function that uses `xarray.apply_ufunc` to perform computations in a vectorized and parallelized manner, and `FACTORS` which determine the extent of each downsampled level. For instance, a factor of '2' halves the number of time samples, '4' reduces them to a quarter, and so on.\n",
    "\n",
    "To each chunk of data, our custom `apply_downsample` function applies the `MinMaxLTTBDownsampler` from the `tsdownsample` library, which selects data points that best represent the overall shape of the signal. This method is particularly effective in preserving the visual integrity of the data, even at reduced resolutions. This may take some time depending on the amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-warning\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Warning</p>\n",
    "    Ensure you have sufficient disk space to handle the new data pyramid prior to running this section. If using the demo data, this will create a ~3 GB file on disk.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The FACTORS will depend on the size of your dataset, available disk size, and the resolution needed\n",
    "# For this demo, we are arbitrarily choosing 8 levels and scaling by quadrupling each preceding factor.\n",
    "FACTORS = [4**i for i in range(8)]\n",
    "\n",
    "def _help_downsample(data, time, n_out):\n",
    "    \"\"\"\n",
    "    Helper function for downsampling and returning as a specific format.\n",
    "    \"\"\"\n",
    "    indices = MinMaxLTTBDownsampler().downsample(time, data, n_out=n_out)\n",
    "    return data[indices], indices\n",
    "\n",
    "\n",
    "def apply_downsample(ts_ds, factor, dims):\n",
    "    \"\"\"\n",
    "    Apply downsampling to a time series dataset.\n",
    "    \"\"\"\n",
    "    dim = dims[0]\n",
    "    n_out = len(ts_ds[\"data\"]) // factor\n",
    "    print(f\"Downsampling by factor {factor} for a size of {n_out}.\")\n",
    "    ts_ds_downsampled, indices = xr.apply_ufunc(\n",
    "        _help_downsample,\n",
    "        ts_ds[\"data\"],\n",
    "        ts_ds[dim],\n",
    "        kwargs=dict(n_out=n_out),\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        output_core_dims=[[dim], [\"indices\"]],\n",
    "        exclude_dims=set((dim,)),\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs=dict(output_sizes={dim: n_out, \"indices\": n_out}),\n",
    "    )\n",
    "    # Update the dimension coordinates with the downsampled indices\n",
    "    ts_ds_downsampled[dim] = ts_ds[dim].isel(time=indices.values[0])\n",
    "    return ts_ds_downsampled.rename(\"data\")\n",
    "\n",
    "\n",
    "if not PYRAMID_PATH.exists():\n",
    "    print(f'Creating Pyramid: {PYRAMID_PATH}')\n",
    "    ts_dt = pyramid_create(\n",
    "        ts_ds,\n",
    "        factors=FACTORS,\n",
    "        dims=[\"time\"],\n",
    "        func=apply_downsample,\n",
    "        type_label=\"pick\",\n",
    "        method_label=\"pyramid_downsample\",\n",
    "    )\n",
    "    display(ts_dt)\n",
    "else:\n",
    "    print(f'Pyramid Already Exists: {PYRAMID_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Pyramid\n",
    "\n",
    "Now we can easily persist the multi-level pyramid `to_zarr` format on disk. This may take some time depending on the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not PYRAMID_PATH.exists():\n",
    "    PYRAMID_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ts_dt.to_zarr(PYRAMID_PATH, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Pyramid Plotting\n",
    "\n",
    "Now that we've created our data pyramid, we can set up the interactive visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start fresh and get a handle to the data pyramid that we've saved to disk, making sure to specify the `zarr` engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dt = open_datatree(PYRAMID_PATH, engine=\"zarr\")\n",
    "ts_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you expand the 'Group' dropdown above, you can see each pyramid level has the same number of channels, but different number of timestamps, since the time dimension was downsampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "First, we will prepare some metadata needed for plotting and define a helper function to extract a dataset at a specific pyramid level and channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ds(ts_dt, level, channels=None):\n",
    "    \"\"\" Extract a dataset at a specific level\"\"\"\n",
    "    ds = ts_dt[str(level)].ds\n",
    "    return ds if channels is None else ds.sel(channel=channels)\n",
    "\n",
    "# Grab the timestamps from the coursest level of the datatree for initialization\n",
    "num_levels = len(ts_dt)\n",
    "coarsest_level = str(num_levels-1)\n",
    "time_da = _extract_ds(ts_dt, coarsest_level)[\"time\"]\n",
    "channels = ts_dt[coarsest_level].ds[\"channel\"].values\n",
    "num_channels = len(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dynamic Plot\n",
    "\n",
    "Now for the main show, we'll utilize a HoloViews `DynamicMap` which will call a custom function called `rescale` whenever there is a change in the visible axes' ranges (`RangeXY`) or the size of a plot (`PlotSize`).\n",
    "\n",
    "Based on the changes and thresholds, a new plot is created using a new subset of the datatree pyramid.\n",
    "\n",
    "\n",
    "<details> <summary> <strong>Want more details? </strong> </summary>\n",
    "\n",
    "When the `rescale` function is triggered, it will first determine which pyramid `zoom_level` has the next closest number of data samples in the visible time range (`time_slice`) compared with the number of horizontal pixels on the screen.\n",
    "\n",
    "Depending on the determined `zoom_level`, data corresponding to the visible time range is fetched through the `_extract_ds` function, which accesses the specific slice of data from the appropriate pyramid level.\n",
    "\n",
    "Finally, for each channel within the specified range, a `Curve` element is generated using HoloViews, and each curve is added to the `Overlay` for a stacked multichannel timeseries visualization.\n",
    "\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PADDING = 0.2  # buffer x-range to reduce update latency with pans and zoom-outs\n",
    "\n",
    "amplitude_dim = hv.Dimension(\"amplitude\", unit=\"µV\")\n",
    "time_dim = hv.Dimension(\"time\", unit=\"s\") # match the index name in the df\n",
    "\n",
    "def rescale(x_range, y_range, width, scale, height):\n",
    "\n",
    "    # Handle case of stream initialization\n",
    "    if x_range is None:\n",
    "        x_range = time_da.min().item(), time_da.max().item()\n",
    "    if y_range is None:\n",
    "        y_range = 0, num_channels\n",
    "\n",
    "    # define data range slice\n",
    "    x_padding = (x_range[1] - x_range[0]) * X_PADDING\n",
    "    time_slice = slice(x_range[0] - x_padding, x_range[1] + x_padding)\n",
    "    channel_slice = slice(y_range[0], y_range[1])\n",
    "\n",
    "    # calculate the appropriate pyramid level and size\n",
    "    if width is None or height is None:\n",
    "        pyramid_level = num_levels - 1\n",
    "        size = time_da.size\n",
    "    else:\n",
    "        sizes = np.array([\n",
    "            _extract_ds(ts_dt, pyramid_level)[\"time\"].sel(time=time_slice).size\n",
    "            for pyramid_level in range(num_levels)\n",
    "        ])\n",
    "        diffs = sizes - width\n",
    "        pyramid_level = np.argmin(np.where(diffs >= 0, diffs, np.inf)) # nearest higher-resolution level\n",
    "        # pyramid_level = np.argmin(np.abs(np.array(sizes) - width)) # nearest (higher or lower resolution) level\n",
    "        size = sizes[pyramid_level]\n",
    "    \n",
    "    title = (\n",
    "        f\"[Pyramid Level {pyramid_level} ({x_range[0]:.2f}s - {x_range[1]:.2f}s)]   \"\n",
    "        f\"[Time Samples: {size}]  [Plot Size WxH: {width}x{height}]\"\n",
    "    )\n",
    "\n",
    "    # extract new data and re-paint the plot\n",
    "    ds = _extract_ds(ts_dt, pyramid_level, channels).sel(time=time_slice, channel=channel_slice).load()\n",
    "\n",
    "    curves = {}\n",
    "    for channel in ds[\"channel\"].values.tolist():\n",
    "        curves[str(channel)] = hv.Curve(ds.sel(channel=channel), [time_dim], ['data'], label=str(channel)).redim(\n",
    "            data=amplitude_dim).opts(\n",
    "            color=\"black\",\n",
    "            line_width=1,\n",
    "            subcoordinate_y=True,\n",
    "            subcoordinate_scale=4,\n",
    "            hover_tooltips = [\n",
    "                (\"channel\", \"$label\"),\n",
    "                (\"time\"),\n",
    "                (\"amplitude\")],\n",
    "            tools=[\"xwheel_zoom\"],\n",
    "            active_tools=[\"box_zoom\"],\n",
    "        )\n",
    "        \n",
    "    curves_overlay = hv.NdOverlay(curves, kdims=\"Channel\", sort=False).opts(\n",
    "            xlabel=\"Time (s)\",\n",
    "            ylabel=\"Channel\",\n",
    "            title=title,\n",
    "            show_legend=False,\n",
    "            padding=0,\n",
    "            min_height=600,\n",
    "            responsive=True,\n",
    "            framewise=True,\n",
    "            axiswise=True,\n",
    "        )\n",
    "    return curves_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_stream = hv.streams.RangeXY()\n",
    "size_stream = hv.streams.PlotSize()\n",
    "dmap = hv.DynamicMap(rescale, streams=[size_stream, range_stream])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist in navigating the dataset, we integrate a minimap widget. This secondary minimap plot provides a condensed overview of the entire dataset, allowing users to select and zoom into areas of interest quickly in the main plot while maintaining the contextualization of the zoomed out view.\n",
    "\n",
    "We will employ datashader rasterization of the image for the minimap plot to display a browser-friendly, aggregated view of the entire dataset. Read more about datashder rasterization via HoloViews [here](https://holoviews.org/user_guide/Large_Data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "\n",
    "y_positions = range(num_channels)\n",
    "yticks = [(i, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "z_data = zscore(ts_dt[coarsest_level].ds[\"data\"].values, axis=1)\n",
    "\n",
    "minimap = rasterize(\n",
    "    hv.QuadMesh((time_da, y_positions, z_data), [\"Time\", \"Channel\"], \"Amplitude\")\n",
    ")\n",
    "\n",
    "minimap = minimap.opts(\n",
    "    cnorm='eq_hist',\n",
    "    cmap=\"RdBu_r\",\n",
    "    alpha=0.5,\n",
    "    xlabel=\"\",\n",
    "    yticks=[yticks[0], yticks[-1]],\n",
    "    toolbar=\"disable\",\n",
    "    height=120,\n",
    "    responsive=True,\n",
    ")\n",
    "\n",
    "tool_link = RangeToolLink(\n",
    "    minimap,\n",
    "    dmap,\n",
    "    axes=[\"x\", \"y\"],\n",
    "    boundsy=(-0.5, len(channels) // 1.25),\n",
    ")\n",
    "\n",
    "app = (dmap + minimap).cols(1)\n",
    "dmap.event(x_range=(5140, 5190)) # trigger an initial range event to get the appropriate pyramid layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p style=\"font-weight:bold; font-size: 16px;\">⚠️ Heads up!</p>\n",
    "    <p>Viewing this on <code>examples.holoviz.org</code>? This is a static version of the notebook 📄. Interactive features that require a live Python process, like downsampling and datashading, won’t work here 🚫. To use these features, open the notebook with a live Python session.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pn.Row(app).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative; display: inline-block;\">\n",
    "    <img src=\"./assets/large_multichan-ts_static.png\" alt=\"Static Preview\" style=\"display: block;\">\n",
    "    <div style=\"\n",
    "        position: absolute;\n",
    "        top: 0;\n",
    "        left: 0;\n",
    "        width: 100%;\n",
    "        height: 100%;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        font-size: 40px;\n",
    "        color: rgba(255, 255, 255, 0.7);\n",
    "        background-color: rgba(0, 0, 0, 0.25);\n",
    "        text-align: center;\n",
    "        border-radius: 5px;\n",
    "    \">\n",
    "        Static Preview\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Next?\n",
    "\n",
    "- Review the [Time Range Annotation Extension](./time_range_annotation.ipynb) to create, edit, and save start/end times, and view categorized ranges overlaid on the multichannel timeseries plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Resources\n",
    "\n",
    "| What? | Why? |\n",
    "| --- | --- |\n",
    "| [xarray Datatree Docs](https://docs.xarray.dev/en/stable/generated/xarray.DataTree.html) | Reference the API for Datatree |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
