{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seattle Lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Lidar Scattered Point Elevation Data\n",
    "\n",
    "This notebook uses Datashader to visualize Lidar elevation data from [the Puget Sound Lidar consortium](http://pugetsoundlidar.ess.washington.edu/), a source of Lidar data for the Puget Sound region of the state of Washington, U.S.A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidar Elevation Data\n",
    "\n",
    "Example X,Y,Z scattered point elevation data from the unpacked 7zip files (unpacked as .gnd files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "! head ../data/q47122d2101.gnd\n",
    "```\n",
    "```\n",
    "X,Y,Z\n",
    "1291149.60,181033.64,467.95\n",
    "1291113.29,181032.53,460.24\n",
    "1291065.38,181035.74,451.41\n",
    "1291113.16,181037.32,455.51\n",
    "1291116.68,181037.42,456.20\n",
    "1291162.42,181038.90,467.81\n",
    "1291111.90,181038.15,454.89\n",
    "1291066.62,181036.73,451.41\n",
    "1291019.10,181035.20,451.64\n",
    "```\n",
    "\n",
    "The Seattle area example below loads 25 `.gnd` elevation files like the one above. We'll download, cache and read the data using `intake`.\n",
    "\n",
    "**NOTE**: Downloading the data for the first time takes about 2 mins due to its large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('./catalog.yml')\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar = cat.seattle_lidar()\n",
    "df = lidar.to_dask()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Metadata\n",
    "\n",
    "Since the data are geographic, we need to know the coordinate reference system (CRS) of the X and Y. All the geographic metadata is stored in the data source entry in the intake catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar.metadata['crs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs('epsg:2855','epsg:3857') # Washington State Plane North EPSG code and Mercator projection EPSG code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_2_M = 0.3048 # conversion factor from feet to meters\n",
    "\n",
    "def convert_coords(df):\n",
    "    lon, lat = transformer.transform(df.X.values * FT_2_M, df.Y.values * FT_2_M)\n",
    "    df['meterswest'], df['metersnorth'] = lon, lat\n",
    "    return df[['meterswest', 'metersnorth', 'Z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the convert_coords function on a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_coords(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the coordinates\n",
    "\n",
    "Since our real dataset is large and partitioned using dask, we need to think about how to apply the `convert_coords` function to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed\n",
    "import dask.delayed\n",
    "from dask.config import set\n",
    "\n",
    "set({'distributed.worker.memory.target': 0.8,\n",
    "     'distributed.worker.memory.spill': 0.9,\n",
    "     'distributed.worker.memory.pause': 0.95,\n",
    "     'distributed.worker.memory.terminate': 0.98})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.distributed.Client(memory_limit='12GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the task graph to figure out a performant way to split up the coords conversion. First we'll try with using `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.delayed(convert_coords)(df).visualize(engine='cytoscape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even though we thought `dask.delayed` would help, in actuality we would be requiring all the processes to be done first and then the conversion would happen on the whole dataset in one go. Another approach would be to use `dask.map_partitions` to do the conversion on each piece of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merc = df.map_partitions(convert_coords)\n",
    "df_merc.visualize(engine='cytoscape', tasks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up the task graph, we can use `df_merc` directly to do the computations on the fly. However, since this dataset fits in memory on my computer, I will do the computation and keep the output in memory for quick use when plotting. \n",
    "\n",
    "**NOTE:** This next cell takes about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = df_merc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the data doesn't fit in memory on your machine, try downsampling the data from each file to only keep 1/100th of the total data. To avoid unnecessary computation, it is better to do the downsampling first and _then_ convert the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = df.sample(frac=0.01).map_partitions(convert_coords)\n",
    "small.visualize(tasks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "small_dataset = small.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews import opts\n",
    "import hvplot.dask # noqa\n",
    "import hvplot.pandas # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the exploration of the time required to display different data, define a function that accepts a regular `pandas` dataframe or a `dask` delayed dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, **kwargs):\n",
    "    \"\"\"Plot point elevation data, rasterizing by mean elevation\"\"\"\n",
    "    image_opts = opts.Image(tools=['hover'], cmap='blues_r', colorbar=True,\n",
    "                            width=800, height=800, xaxis=None, yaxis=None)\n",
    "    return data.hvplot.points('meterswest', 'metersnorth', color='Z', rasterize=True,\n",
    "                              aggregator='mean', tiles='EsriImagery', **kwargs).opts(image_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will construct the plot using the `small_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(small_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot(small_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot(df_merc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Raster of Point Data\n",
    "\n",
    "If we compute a raster of the point data then we don't need to store as much data in memory, which should allow faster interactions, and allow use with lots of other tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raster = plot(dataset, dynamic=False, width=1000, height=1000).data[('Image', 'I')].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an `xarray.Dataset` with x and y coordinates and a 2D array of Z values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.metersnorth[1] - raster.metersnorth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these data we can use the geo tools in datashader to compute and visualize the elevation using hillshading for instance. See [Datashader User Guide #11](https://github.com/holoviz/datashader/blob/main/examples/user_guide/11_Geography.ipynb) for more datashader and xrspatial geo tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from xrspatial import hillshade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illuminated = hillshade(raster.get('meterswest_metersnorth Z'))\n",
    "hv_shaded = hv.Image((raster.meterswest, raster.metersnorth, illuminated))\n",
    "tiles = gv.tile_sources.EsriImagery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles * rasterize(hv_shaded.options(opts.Image(cmap='blues', height=600, width=600)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "It'd be nice to have a `rasterio` writer from `xarray` so that we could easily write chunked geotiffs from `xarray` objects.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "raster.to_rasterio(path=None, mode='w', compute=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
