{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 US Census data\n",
    "\n",
    "*Say something about the data, source, and objective of this analysis here*\n",
    "\n",
    "\n",
    "### Load data and set up\n",
    "\n",
    "The census data has been saved in a [Parquet](https://parquet.apache.org)-format file, which can be loaded into a columnar data structure like a [Pandas](https://pandas.pydata.org) or [Dask](https://dask.pydata.org) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds, datashader.transfer_functions as tf, numpy as np\n",
    "import dask.dataframe as dd\n",
    "import intake\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 ms, sys: 4.22 ms, total: 31.4 ms\n",
      "Wall time: 28.7 ms\n"
     ]
    },
    {
     "data": {
      "application/yaml": "us_census_data:\n  args:\n    key: ASIAZBQVV7KSO3VUJASY\n    secret: AChJ4xk8F+tf11JUgj3+DmMg9+JjpNOvFzx1wwi5\n    storage_options:\n      anon: false\n    token: IQoJb3JpZ2luX2VjEOz//////////wEaCXVzLWVhc3QtMSJIMEYCIQDMB/sqlqKyfqrU76sD30LZHV0pCXneFKMybzpkmknm5wIhAMx4s56xqSe/QUUfPoQCg5UJbUUkmPHX9v6v9A29swHTKoQDCDUQABoMNjIxNzQxOTk2NzA4IgwHCKC7A25tLf6/pBAq4QKXpVnIMWFlS0AG3ZSUgUABiIQ1+x0RNc2mHtTqeqbhwQkXaTjSwf4H5BVvMJu2M3TGlzx9u9PHMsSB4LfCaUNZqexKpGOIjQUIGPOWbxFPWXXp6ZWw8veMWvaQ4ZcxmhdSSn2XnuNTyMJojBVL7nogQO4xE9K0ADrXFkh/ZGVVnOGEyuq1+yn7q1WhKFk76PscYD+13UiCkNCohBrWbXodKXjUSsXv3TBzGy5d5ux6f2nAsS/DJRVmRG/WvKi34d9N7Y1wIbELui5a3T3e8qP520rPNdkh6UbXnJm8uoqdnDidEILEYDGrLEd/ykB9WybQEar49waJKdT5LewUi6omqWz7J4i/j3QiUNVDMpPQtXiUonJ16ZFtQ3iVWVNT28TCetXq1rBMhBGM/hCqRWRgOABCYtD0OPIDHgU8XweFfWk7MKFJpqHKpNmoKWh03TurTHZHjKTjJRvi5gVW1Mll1TD5k9y3BjqlAWlL+c1/7uCY4R7g5Q85p8ti828vVNFH3IGlHKRPvRIiFE0JOBUIUta5edSq/slHS/5x242KPrIAZTgZUR3CmdXpyS0oDshmTzQPWBKFHQYIpicCA2JLc/euMb3Joei7N1SGkMiUdDBGl3BLXCb5+ZwjpqrkhylPLoyNUyhHTd8naY49fg3f9PTl8zyWt1RVo9FCTlKf52StS8LciVUAIx5GHEQRbw==\n    urlpath: s3://xtechfinals/data/synthetic_people.parquet.zip\n  description: ''\n  driver: intake_parquet.source.ParquetSource\n  metadata:\n    cache:\n    - argkey: urlpath\n      dest: cache/census2020.zip\n      method: download\n      regex: s3://xtechfinals\n      type: file\n    catalog_dir: /Users/mac/Documents/development/HOLOVIZ_REP/examples/census2020/./\n    description: US 2020 census data in Parquet format\n",
      "text/plain": [
       "us_census_data:\n",
       "  args:\n",
       "    key: ASIAZBQVV7KSO3VUJASY\n",
       "    secret: AChJ4xk8F+tf11JUgj3+DmMg9+JjpNOvFzx1wwi5\n",
       "    storage_options:\n",
       "      anon: false\n",
       "    token: IQoJb3JpZ2luX2VjEOz//////////wEaCXVzLWVhc3QtMSJIMEYCIQDMB/sqlqKyfqrU76sD30LZHV0pCXneFKMybzpkmknm5wIhAMx4s56xqSe/QUUfPoQCg5UJbUUkmPHX9v6v9A29swHTKoQDCDUQABoMNjIxNzQxOTk2NzA4IgwHCKC7A25tLf6/pBAq4QKXpVnIMWFlS0AG3ZSUgUABiIQ1+x0RNc2mHtTqeqbhwQkXaTjSwf4H5BVvMJu2M3TGlzx9u9PHMsSB4LfCaUNZqexKpGOIjQUIGPOWbxFPWXXp6ZWw8veMWvaQ4ZcxmhdSSn2XnuNTyMJojBVL7nogQO4xE9K0ADrXFkh/ZGVVnOGEyuq1+yn7q1WhKFk76PscYD+13UiCkNCohBrWbXodKXjUSsXv3TBzGy5d5ux6f2nAsS/DJRVmRG/WvKi34d9N7Y1wIbELui5a3T3e8qP520rPNdkh6UbXnJm8uoqdnDidEILEYDGrLEd/ykB9WybQEar49waJKdT5LewUi6omqWz7J4i/j3QiUNVDMpPQtXiUonJ16ZFtQ3iVWVNT28TCetXq1rBMhBGM/hCqRWRgOABCYtD0OPIDHgU8XweFfWk7MKFJpqHKpNmoKWh03TurTHZHjKTjJRvi5gVW1Mll1TD5k9y3BjqlAWlL+c1/7uCY4R7g5Q85p8ti828vVNFH3IGlHKRPvRIiFE0JOBUIUta5edSq/slHS/5x242KPrIAZTgZUR3CmdXpyS0oDshmTzQPWBKFHQYIpicCA2JLc/euMb3Joei7N1SGkMiUdDBGl3BLXCb5+ZwjpqrkhylPLoyNUyhHTd8naY49fg3f9PTl8zyWt1RVo9FCTlKf52StS8LciVUAIx5GHEQRbw==\n",
       "    urlpath: s3://xtechfinals/data/synthetic_people.parquet.zip\n",
       "  description: ''\n",
       "  driver: intake_parquet.source.ParquetSource\n",
       "  metadata:\n",
       "    cache:\n",
       "    - argkey: urlpath\n",
       "      dest: cache/census2020.zip\n",
       "      method: download\n",
       "      regex: s3://xtechfinals\n",
       "      type: file\n",
       "    catalog_dir: /Users/mac/Documents/development/HOLOVIZ_REP/examples/census2020/./\n",
       "    description: US 2020 census data in Parquet format\n"
      ]
     },
     "metadata": {
      "application/json": {
       "root": "us_census_data"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cat = intake.open_catalog('./catalog.yml')\n",
    "census_data = cat.us_census_data(\n",
    "    key=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    secret=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    token=os.getenv('AWS_SESSION_TOKEN')\n",
    ")\n",
    "#census_data = cat.us_census_data()\n",
    "census_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "The provided token has expired.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/s3fs/core.py:723\u001b[0m, in \u001b[0;36mS3FileSystem._lsdir\u001b[0;34m(self, path, refresh, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    722\u001b[0m files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterdir(\n\u001b[1;32m    724\u001b[0m     bucket,\n\u001b[1;32m    725\u001b[0m     max_items\u001b[38;5;241m=\u001b[39mmax_items,\n\u001b[1;32m    726\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    727\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    728\u001b[0m     versions\u001b[38;5;241m=\u001b[39mversions,\n\u001b[1;32m    729\u001b[0m ):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/s3fs/core.py:773\u001b[0m, in \u001b[0;36mS3FileSystem._iterdir\u001b[0;34m(self, bucket, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    766\u001b[0m it \u001b[38;5;241m=\u001b[39m pag\u001b[38;5;241m.\u001b[39mpaginate(\n\u001b[1;32m    767\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m    768\u001b[0m     Prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw,\n\u001b[1;32m    772\u001b[0m )\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m i\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommonPrefixes\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/aiobotocore/paginate.py:30\u001b[0m, in \u001b[0;36mAioPageIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(current_kwargs)\n\u001b[1;32m     31\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_parsed_response(response)\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/aiobotocore/client.py:412\u001b[0m, in \u001b[0;36mAioBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    411\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the ListObjectsV2 operation: The provided token has expired.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcensus_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/intake_parquet/source.py:107\u001b[0m, in \u001b[0;36mParquetSource.to_dask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dask\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_urlpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/dask/backends.py:122\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:473\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    471\u001b[0m     index \u001b[38;5;241m=\u001b[39m [index]\n\u001b[0;32m--> 473\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m meta, statistics, parts, index \u001b[38;5;241m=\u001b[39m read_metadata_result[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:342\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_metadata\u001b[39m(\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dd_meta(dataset_info)\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:806\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Case-dependent pyarrow.dataset creation\u001b[39;00m\n\u001b[1;32m    805\u001b[0m has_metadata_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    807\u001b[0m \n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# Use _analyze_paths to avoid relative-path\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;66;03m# problems (see GH#5608)\u001b[39;00m\n\u001b[1;32m    810\u001b[0m     paths, base, fns \u001b[38;5;241m=\u001b[39m _sort_and_analyze_paths(paths, fs)\n\u001b[1;32m    811\u001b[0m     paths \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39msep\u001b[38;5;241m.\u001b[39mjoin([base, fns[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/s3fs/core.py:1483\u001b[0m, in \u001b[0;36mS3FileSystem._isdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# This only returns things within the path and NOT the path object itself\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lsdir(path))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/development/HOLOVIZ_REP/examples/census2020/envs/default/lib/python3.10/site-packages/s3fs/core.py:736\u001b[0m, in \u001b[0;36mS3FileSystem._lsdir\u001b[0;34m(self, path, refresh, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    734\u001b[0m     files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dirs\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m translate_boto_error(e)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delimiter \u001b[38;5;129;01mand\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m versions:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdircache[path] \u001b[38;5;241m=\u001b[39m files\n",
      "\u001b[0;31mPermissionError\u001b[0m: The provided token has expired."
     ]
    }
   ],
   "source": [
    "df = census_data.to_dask()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call ``.persist()`` to use Dask's fast in-core operations, but if you have less than 16GB of RAM, you can omit that line to use Dask's support for out-of-core (larger than memory) operation. Working out of core will be much slower, but should work even on small machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are *** rows in this dataframe (one per person counted in the census), each with a location in Web Mercator format and a race encoded as a single character (where 'w' is white, 'b' is black, 'a' is Asian, 'h' is Hispanic, and 'o' is other (typically Native American)). (Try ``len(df)`` to see the size, if you want to check, though that always forces the dataset to be loaded so it's skipped here.)\n",
    "\n",
    "Let's define some geographic ranges to look at later, and also a default plot size.  Feel free to increase `plot_width` to 2000 or more if you have a very large monitor or want to save big files to disk, which shouldn't *greatly* affect the processing time or memory requirements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA           = ((-124.72,  -66.95), (23.55, 50.06))\n",
    "LakeMichigan  = (( -91.68,  -83.97), (40.75, 44.08))\n",
    "Chicago       = (( -88.29,  -87.30), (41.57, 42.00))\n",
    "Chinatown     = (( -87.67,  -87.63), (41.84, 41.86))\n",
    "NewYorkCity   = (( -74.39,  -73.44), (40.51, 40.91))\n",
    "LosAngeles    = ((-118.53, -117.81), (33.63, 33.96))\n",
    "Houston       = (( -96.05,  -94.68), (29.45, 30.11))\n",
    "Austin        = (( -97.91,  -97.52), (30.17, 30.37))\n",
    "NewOrleans    = (( -90.37,  -89.89), (29.82, 30.05))\n",
    "Atlanta       = (( -84.88,  -84.04), (33.45, 33.84))\n",
    "\n",
    "from datashader.utils import lnglat_to_meters as webm\n",
    "x_range,y_range = [list(r) for r in webm(*USA)]\n",
    "\n",
    "plot_width  = int(900)\n",
    "plot_height = int(plot_width*7.0/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also choose a background color for our results.  A black background makes bright colors more vivid, and works well when later adding relatively dark satellite image backgrounds, but white backgrounds (`background=None`) are good for examining the weakest patterns, and work well when overlaying on maps that use light colors.  Try it both ways and decide for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = \"black\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some utility functions and colormaps, and to make the page as big as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datashader.utils import export_image\n",
    "from datashader.colors import colormap_select, Greys9\n",
    "\n",
    "export = partial(export_image, background = background, export_path=\"export\")\n",
    "cm = partial(colormap_select, reverse=(background!=\"black\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population density\n",
    "\n",
    "For our first examples, let's ignore the race data for now, focusing on population density alone (as for the [NYC taxi](https://examples.holoviz.org/nyc_taxi) example).  We'll first aggregate all the points from the continental USA into a grid containing the population density per pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cvs = ds.Canvas(plot_width, plot_height, *webm(*USA))\n",
    "agg = cvs.points(df, 'easting', 'northing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing this aggregate grid will take some CPU power (1 second on a MacBook Pro), because datashader has to iterate through the entire dataset, with hundreds of millions of points.  Once the `agg` array has been computed, subsequent processing will now be nearly instantaneous, because there are far fewer pixels on a screen than points in the original database.\n",
    "\n",
    "If we now plot the aggregate grid linearly, we can clearly see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(agg, cmap = cm(Greys9), how='linear'),\"census_gray_linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...almost nothing.  If you know what to look for, you can see hotspots (high population densities) in New York City, Los Angeles, Chicago, and a few other places.  More hotspots can dimly be seen when using a white background than with a black, on most monitors, though they are very dim either way. In any case, for feeding 300 million points in, we're getting almost nothing back in terms of visualization.\n",
    "\n",
    "The first thing we can do is to prevent undersampling (see [plotting pitfalls](https://datashader.org/user_guide/Plotting_Pitfalls.html)).  In the plot above, there is no way to distinguish between pixels that are part of the background, and those that have low but nonzero counts; both are mapped to black or nearly black on a linear scale.  Instead, let's map all values that are not background to a dimly visible gray, leaving the highest-density values at white.  I.e., let's discard the first 25% of the gray colormap, and linearly interpolate the population densities over the remaining range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(agg, cmap = cm(Greys9,0.25), how='linear'),\"census_gray_linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot reveals at least that data has been measured only within the political boundaries of the continental United States, and also that many areas in the West are so poorly populated that many pixels contained not even a single person.  (In datashader images, the background color is shown for pixels that have no data at all, using the alpha channel of a PNG image, while the colormap range is shown for pixels that do have data.)  Some additional population centers are now visible, at least on some monitors. But mainly what the above plot indicates is that population in the USA is extremely non-uniformly distributed, with hotspots in a few regions, and nearly all other pixels having much, much lower (but nonzero) values. Again, that's not much information to be getting out out of 300 million datapoints!\n",
    "\n",
    "The problem is that of the available intensity scale in this gray colormap, nearly all pixels are colored the same low-end gray value, with only a few urban areas using any other colors.  Thus this version of the map conveys very little information as well.  Because the data are clearly distributed so non-uniformly, let's instead try a nonlinear mapping from population counts into the colormap.  A logarithmic mapping is often a good choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(agg, cmap = cm(Greys9,0.2), how='log'),\"census_gray_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddenly, we can see an amazing amount of structure!  There are clearly meaningful patterns at nearly every location, ranging from the geographic variations in the mountainous West, to the densely spaced urban centers in New England, and the many towns stretched out along roadsides in the midwest (especially those leading to Denver, the hot spot towards the right of the Rocky Mountains).  \n",
    "\n",
    "Clearly, we can now see much more of what's going on in this dataset, thanks to the logarithmic mapping.  Worryingly, though, the choice of `'log'` was purely arbitrary, basically just a hunch based on how typical datasets behave.  One could easily imagine that other nonlinear functions would show other interesting patterns, or that different functions would be needed for different datasets.  \n",
    "\n",
    "Instead of blindly searching through the space of all such functions, we can step back and notice that the main effect of the log transform has been to reveal *local* patterns at all population densities -- small towns show up clearly even if they are just slightly more dense than their immediate, rural neighbors, yet large cities with high population density also show up well against the surrounding suburban regions, even if those regions are more dense than the small towns on an absolute scale.\n",
    "\n",
    "With this idea of showing relative differences across a large range of data values in mind, let's try the image-processing technique called histogram equalization. I.e., given a set of raw counts, map these into a range for display such that every available color on the screen represents about the same number of samples in the original dataset.  The result is similar to that from the log transform, but is now non-parametric -- it will equalize any linearly or nonlinearly distributed integer data, regardless of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(agg, cmap = cm(Greys9,0.2), how='eq_hist'),\"census_gray_eq_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Histogram equalization also works for non-integer data, but in that case it will use a finite set of bins to divide the interval between the minimum and maximum values, and will thus not be able to normalize the histogram perfectly for highly non-uniform distributions.)  Effectively, this transformation converts the data from raw magnitudes, which can easily span a much greater range than the dynamic range visible to the eye, to a rank-order or percentile representation, which reveals density differences at all ranges but obscures the absolute magnitudes that were used to compute the ordering. In this representation, you can clearly see the effects of geography (rivers, coastlines, and mountains) on the population density, as well as history (denser near the longest-populated areas), and even infrastructure (with many small towns located at crossroads).\n",
    "\n",
    "Given the very different results from the different types of plot, a good practice when visualizing any dataset with datashader is to look at both the linear and the histogram-equalized versions of the data; the linear version preserves the magnitudes, but obscures the distribution, while the histogram-equalized version reveals the distribution while preserving only the order of the magnitudes, not their actual values.  If both plots are similar, then the data is distributed nearly uniformly across the interval.  But much more commonly, the distribution will be highly nonlinear, and the linear plot will reveal only the envelope of the data, i.e., the lowest and the highest values.  In such cases, the histogram-equalized plot will reveal much more of the structure of the data, because it maps the local patterns in the data into perceptible color differences on the screen.\n",
    "\n",
    "Because we are only plotting a single dimension, we can use the colors of the display to effectively reach a higher dynamic range, mapping ranges of data values into different color ranges. When doing so, it is crucial to use a perceptually uniform colormap (see [this paper](https://arxiv.org/pdf/1509.03700v1.pdf) for more details). Here we'll use the \"fire\" colormap from the [colorcet](https://github.com/holoviz/colorcet) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorcet import fire\n",
    "export(tf.shade(agg, cmap = cm(fire,0.2), how='eq_hist'),\"census_ds_fire_eq_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should reveal even more of the detail than the gray colormaps above.  \n",
    "\n",
    "You can also import colormaps directly from `matplotlib.cm` or `bokeh.palettes`, but only a few of those will be perceptually uniform.  For instance, matplotlib's \"hot\" colormap is similar to colorcet's \"fire\", but reveals far less of the detail available in this data because it has long stretches of perceptually equivalent colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import hot\n",
    "export(tf.shade(agg, cmap = hot, how='eq_hist'),\"census_ds_mhot_eq_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the ``cm()`` function used here that lets us switch backgrounds and prevent undersampling only supports Bokeh palettes (either provided by Bokeh or from cetcolors), though we do provide Bokeh-palette versions of matplotlib's perceptually uniform `viridis` and `inferno` colormaps from within datashader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datashader.colors import viridis\n",
    "export(tf.shade(agg, cmap=cm(viridis), how='eq_hist'),\"census_viridis_eq_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting magnitudes of this type, it should be safe to use any of the \"linear\" colormaps from the [colorcet](https://github.com/holoviz/colorcet) package, depending on your preference.\n",
    "\n",
    "Colormaps can also be used to address very specific questions about the data itself.  For instance, after histogram equalization, data should be uniformly distributed across the visible colormap.  Thus if we want to highlight e.g. the top 1% of pixels, by population density, we can use a colormap divided into 100 ranges, and just change the top one to a different color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grays2 = cm([(i,i,i) for i in np.linspace(0,255,99)])\n",
    "grays2 += [\"red\"]\n",
    "export(tf.shade(agg, cmap = grays2, how='eq_hist'),\"census_gray_redhot1_eq_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot now conveys nearly all the information available in the linear plot, i.e. that only a few pixels have the very highest population densities, while also conveying the structure of the data at all population density ranges via histogram equalization.  And in no case was any parameter value chosen based on the data itself, just on the question we wanted to ask about the data, which is crucial for posing and answering questions objectively using visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data (race)\n",
    "\n",
    "Since we've got the racial category for every pixel, we can use color to indicate the category value, instead of just extending dynamic range or highlighting percentiles as above.  To do this, we first need to set up a color key for each category label, with different color keys as appropriate to make colors that stand out against the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if background == \"black\":\n",
    "    color_key = {'w':'aqua', 'b':'lime',  'a':'red', 'h':'fuchsia', 'o':'yellow'}\n",
    "else:\n",
    "    color_key = {'w':'blue', 'b':'green', 'a':'red', 'h':'orange', 'o':'saddlebrown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show a key for those near the end of this notebook if you want to scroll down, but it requires plotting tools not yet introduced. We can now aggregate the counts *per race* into grids, using `ds.by`, instead of just a single grid with the total counts (via the default aggregate reducer `ds.count`), and then generate an image by colorizing each pixel using the aggregate information from each category for that pixel's location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(longitude_range, latitude_range, w=plot_width, h=plot_height):\n",
    "    x_range,y_range=webm(longitude_range,latitude_range)\n",
    "    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "    agg = cvs.points(df, 'easting', 'northing', ds.by('race'))\n",
    "    img = tf.shade(agg, color_key=color_key, how='eq_hist')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the USA is overwhelmingly white in geographic terms, apart from some predominantly Hispanic regions along the Southern border, some regions with high densities of blacks in the Southeast, and a few isolated areas of category \"Other\" in the West (primarily Native American reservation areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "export(create_image(*USA),\"Zoom 0 - USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the racial makeup has some sharp boundaries around urban centers, as we can see if we zoom in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "export(create_image(*LakeMichigan),\"Zoom 1 - Lake Michigan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sufficient zoom, it becomes clear that Chicago (like most large US cities) has both a wide diversity of racial groups, and profound geographic segregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "export(create_image(*Chicago),\"Zoom 2 - Chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we can zoom in far enough to see individual datapoints.  Here we can see that the Chinatown region of Chicago has, as expected, very high numbers of Asian residents, and that other nearby regions (separated by geographic features like rivers, railroads and highways) have other races, varying in how uniformly segregated they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "export(tf.spread(create_image(*Chinatown),px=plot_width//400),\"Zoom 3 - Chinatown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we've used the `tf.spread` function to enlarge each point to cover multiple pixels so that each point is clearly visible, which adds a bit to the computation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other cities, for comparison\n",
    "\n",
    "Different US cities have very different racial makeup, but they all appear highly segregated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*NewYorkCity),\"NYC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*LosAngeles),\"LosAngeles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*Houston),\"Houston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*Atlanta),\"Atlanta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*NewOrleans),\"NewOrleans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(create_image(*Austin),\"Austin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing racial data through visualization\n",
    "\n",
    "In addition to simply visualizing categorical data, we can break those categories down to ask specific questions.  For instance, if we switch back to the full USA and then select only the black population, we can see that blacks predominantly reside in urban areas except in the South and the East Coast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = ds.Canvas(plot_width=plot_width, plot_height=plot_height)\n",
    "aggc = cvs.points(df, 'easting', 'northing', ds.by('race'))\n",
    "\n",
    "export(tf.shade(aggc.sel(race='b'), cmap=cm(Greys9,0.25), how='eq_hist'),\"USA blacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Compare to \"census_gray_eq_hist\" above.)\n",
    "\n",
    "Or we can show only those pixels where there is at least one resident from each of the racial categories white, black, Asian, and Hispanic, which mainly highlights urban areas (compare to \"Zoom 0 - USA\" above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg2 = aggc.where((aggc.sel(race=['w', 'b', 'a', 'h']) > 0).all(dim='race')).fillna(0)\n",
    "export(tf.shade(agg2, color_key=color_key, how='eq_hist'),\"USA all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, the colors still show the racial makeup of each pixel, but the pixels have been filtered so that only those with at least one datapoint from every race are shown.\n",
    "\n",
    "Or we can look at all pixels where there are more black than white datapoints, which highlights predominantly black neighborhoods of large urban areas across most of the USA, but some rural areas and small towns in the South:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(tf.shade(aggc.where(aggc.sel(race='w') < aggc.sel(race='b')).fillna(0), color_key=color_key, how='eq_hist'),\"more_blacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the colors still show the predominant race in that pixel, which is black for many of these, but in Southern California it looks like there are several large neighborhoods where blacks outnumber whites but both are outnumbered by Hispanics.\n",
    "\n",
    "In any case, the thing to do here is to try out your own hypotheses and questions, whether for the USA or for your own region. The aggregate array is just an ordinary xarray multidimensional array, so you can see the [xarray documentation](https://xarray.pydata.org) for how to select and transform that data.  E.g. you can try posing questions that are independent of the number of datapoints in each pixel, since that varies so much geographically, by normalizing the aggregated data in various ways. Now that the data's been aggregated but not yet rendered to the screen, there is an infinite range of queries you can pose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive HoloViews+Bokeh plots overlaid with map data\n",
    "\n",
    "The above plots all show static images on their own.  datashader is independent of Bokeh, HoloViews, or other visualization libraries, but when combined with HoloViews and Bokeh, it is easy to make interactive plots that incorporate maps, satellite imagery, annotations, legends, and hover-tool information.\n",
    "\n",
    "To do this, let's import what we need and set some defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews.element import tiles\n",
    "from holoviews.operation.datashader import datashade, dynspread, rasterize\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "opts.defaults(opts.Overlay(width=900, height=525, xaxis=None, yaxis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datashader doesn't render any text, but now that we have HoloViews and Bokeh, we can make a color key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = {'w':'White', 'b':'Black', 'a':'Asian', 'h':'Hispanic', 'o':'Other'}\n",
    "\n",
    "color_points = hv.NdOverlay({races[k]: hv.Points([webm(-80,40)]).opts(color=v, size=0) for k, v in color_key.items()})\n",
    "color_points.opts(clone=True, xaxis=None, yaxis=None, height=190, width=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an interactive plot using a dynamic wrapper ``datashade`` that calls datashader whenever someone zooms or pans the plot.  In this pipeline, we'll use the `tf.dynspread` function to automatically increase the plotted size of each datapoint, once you've zoomed in so far that datapoints no longer have nearby neighbors.  We'll also add some image tiles in the background, using satellite-derived maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(hv.Dataset(df, kdims=['easting', 'northing'], vdims=['race']))\n",
    "shaded = datashade(points, color_key=color_key, min_alpha=100, aggregator=ds.by('race'))\n",
    "race   = dynspread(shaded, threshold=0.8).opts(bgcolor='black')\n",
    "\n",
    "tiles.EsriImagery().opts(alpha=0.5, bgcolor='black') * race * color_points * tiles.StamenLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need a live copy of the notebook, with a running server; zooming and panning will work for the background tiles but not for the datashaded data if you are viewing a static exported copy like on anaconda.org, datashader.org, or examples.pyviz.org.\n",
    "\n",
    "You can similarly zoom into the population density data (ignoring race):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = dynspread(rasterize(points)).opts(cnorm='eq_hist', cmap=\"fire\", colorbar=True, tools=['hover'])\n",
    "tiles.EsriImagery().opts(alpha=0.5, bgcolor='black') * population * tiles.StamenLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dashboard](https://examples.holoviz.org/datashader_dashboard/dashboard.html) example shows this same census data in the context of an interactive dashboard, including color keys and hover information that help reveal the magnitudes at every location even while the plot faithfully reveals the structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
